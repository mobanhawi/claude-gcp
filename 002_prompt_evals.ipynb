{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5437be1e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T01:22:13.883676Z",
     "start_time": "2025-12-30T01:22:13.819863Z"
    }
   },
   "outputs": [],
   "source": [
    "from anthropic import AnthropicVertex\n",
    "from os import environ\n",
    "\n",
    "region = environ.get(\"CLOUD_ML_REGION\", \"\")\n",
    "project_id = environ.get(\"ANTHROPIC_VERTEX_PROJECT_ID\", \"\")\n",
    "client = AnthropicVertex(region=region, project_id=project_id)\n",
    "model = \"claude-sonnet-4-5@20250929\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b0d8e9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T01:22:13.913564Z",
     "start_time": "2025-12-30T01:22:13.897702Z"
    }
   },
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "from typing import List, Optional\n",
    "\n",
    "\n",
    "def add_user_message(messages: List[dict], content: str):\n",
    "    _add_role_message(messages, \"user\", content)\n",
    "\n",
    "\n",
    "def add_agent_message(messages: List[dict], content: str):\n",
    "    _add_role_message(messages, \"assistant\", content)\n",
    "\n",
    "\n",
    "def _add_role_message(messages: List[dict], role: str, content: str):\n",
    "    messages.append({\"role\": role, \"content\": content})\n",
    "\n",
    "\n",
    "\"\"\" Sends a chat request to the Claude model with the given messages. \"\"\"\n",
    "\n",
    "\n",
    "def chat(\n",
    "    messages: List[dict],\n",
    "    system: Optional[str] = None,\n",
    "    temperature: float = 1.0,\n",
    "    stop_sequences: List[str] = None,\n",
    ") -> str:\n",
    "    params = {\n",
    "        \"model\": model,\n",
    "        \"max_tokens\": 1000,\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": temperature,\n",
    "        \"stop_sequences\": stop_sequences,\n",
    "    }\n",
    "\n",
    "    if system is not None:\n",
    "        params[\"system\"] = system\n",
    "\n",
    "    response = client.messages.create(**params)\n",
    "    return response.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e788701",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T01:22:13.929566Z",
     "start_time": "2025-12-30T01:22:13.920393Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to generate a new dataset\n",
    "import json\n",
    "\n",
    "\n",
    "def generate_dataset():\n",
    "    prompt = \"\"\"\n",
    "Generate a evaluation dataset for a prompt evaluation. The dataset will be used to evaluate prompts\n",
    "that generate Python, JSON, or Regex specifically for AWS-related tasks. Generate an array of JSON objects,\n",
    "each representing task that requires Python, JSON, or a Regex to complete.\n",
    "\n",
    "Example output:\n",
    "```json\n",
    "[\n",
    "    {\n",
    "        \"task\": \"Description of task\",\n",
    "        \"format\": \"json\" or \"python\" or \"regex\",\n",
    "        \"solution_criteria\": \"Key criteria for evaluating the solution\"\n",
    "    },\n",
    "    ...additional\n",
    "]\n",
    "```\n",
    "\n",
    "* Focus on tasks that can be solved by writing a single Python function, a single JSON object, or a regular expression.\n",
    "* Focus on tasks that do not require writing much code\n",
    "\n",
    "Please generate 3 objects.\n",
    "\"\"\"\n",
    "\n",
    "    messages = []\n",
    "    add_user_message(messages, prompt)\n",
    "    add_agent_message(messages, \"```json\")\n",
    "    text = chat(messages, stop_sequences=[\"```\"])\n",
    "    return json.loads(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "438ed743",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T01:22:26.209065Z",
     "start_time": "2025-12-30T01:22:13.930432Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generate the dataset and write it to 'dataset.json'\n",
    "dataset = generate_dataset()\n",
    "with open(\"dataset.json\", \"w\") as f:\n",
    "    json.dump(dataset, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36b89174",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T01:22:26.225131Z",
     "start_time": "2025-12-30T01:22:26.217559Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to grade a test case + output using a model\n",
    "def grade_by_model(test_case, output):\n",
    "    eval_prompt = f\"\"\"\n",
    "You are an expert AWS code reviewer. Your task is to evaluate the following AI-generated solution.\n",
    "\n",
    "Original Task:\n",
    "<task>\n",
    "{test_case[\"task\"]}\n",
    "</task>\n",
    "\n",
    "Solution to Evaluate:\n",
    "<solution>\n",
    "{output}\n",
    "</solution>\n",
    "\n",
    "Criteria you should use to evaluate the solution:\n",
    "<criteria>\n",
    "{test_case[\"solution_criteria\"]}\n",
    "</criteria>\n",
    "\n",
    "Output Format\n",
    "Provide your evaluation as a structured JSON object with the following fields, in this specific order:\n",
    "- \"strengths\": An array of 1-3 key strengths\n",
    "- \"weaknesses\": An array of 1-3 key areas for improvement\n",
    "- \"reasoning\": A concise explanation of your overall assessment\n",
    "- \"score\": A number between 1-10\n",
    "\n",
    "Respond with JSON. Keep your response concise and direct.\n",
    "Example response shape:\n",
    "{{\n",
    "    \"strengths\": string[],\n",
    "    \"weaknesses\": string[],\n",
    "    \"reasoning\": string,\n",
    "    \"score\": number\n",
    "}}\n",
    "    \"\"\"\n",
    "\n",
    "    messages = []\n",
    "    add_user_message(messages, eval_prompt)\n",
    "    add_agent_message(messages, \"```json\")\n",
    "    eval_text = chat(messages, stop_sequences=[\"```\"])\n",
    "    return json.loads(eval_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83809a7e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T01:22:26.252058Z",
     "start_time": "2025-12-30T01:22:26.235454Z"
    }
   },
   "outputs": [],
   "source": [
    "# Passes a test case into Claude\n",
    "def run_prompt(test_case):\n",
    "    prompt = f\"\"\"\n",
    "Please solve the following task:\n",
    "\n",
    "{test_case[\"task\"]}\n",
    "\n",
    "* Respond only with Python, JSON, or a plain Regex\n",
    "* Do not add any comments or commentary or explanation\n",
    "\"\"\"\n",
    "\n",
    "    messages = []\n",
    "    add_user_message(messages, prompt)\n",
    "    add_agent_message(messages, \"```code\")\n",
    "    output = chat(messages, stop_sequences=[\"```\"])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7953c666",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T01:22:26.278200Z",
     "start_time": "2025-12-30T01:22:26.263827Z"
    }
   },
   "outputs": [],
   "source": [
    "# Functions to validate the output structure\n",
    "import re\n",
    "import ast\n",
    "\n",
    "\n",
    "def validate_json(text):\n",
    "    try:\n",
    "        json.loads(text.strip())\n",
    "        return 10\n",
    "    except json.JSONDecodeError:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def validate_python(text):\n",
    "    try:\n",
    "        ast.parse(text.strip())\n",
    "        return 10\n",
    "    except SyntaxError:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def validate_regex(text):\n",
    "    try:\n",
    "        re.compile(text.strip())\n",
    "        return 10\n",
    "    except re.error:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def grade_syntax(response, test_case):\n",
    "    format = test_case[\"format\"]\n",
    "    if format == \"json\":\n",
    "        return validate_json(response)\n",
    "    elif format == \"python\":\n",
    "        return validate_python(response)\n",
    "    else:\n",
    "        return validate_regex(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9bcc4671",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T01:22:26.306851Z",
     "start_time": "2025-12-30T01:22:26.292883Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to execute a single test case and grade the output\n",
    "def run_test_case(test_case):\n",
    "    \"\"\"Calls run_prompt, then grades the result\"\"\"\n",
    "    output = run_prompt(test_case)\n",
    "\n",
    "    model_grade = grade_by_model(test_case, output)\n",
    "    model_score = model_grade[\"score\"]\n",
    "    reasoning = model_grade[\"reasoning\"]\n",
    "\n",
    "    syntax_score = grade_syntax(output, test_case)\n",
    "\n",
    "    score = (model_score + syntax_score) / 2\n",
    "\n",
    "    return {\n",
    "        \"output\": output,\n",
    "        \"test_case\": test_case,\n",
    "        \"score\": score,\n",
    "        \"reasoning\": reasoning,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5fa99d36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T01:22:26.371500Z",
     "start_time": "2025-12-30T01:22:26.307596Z"
    }
   },
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "\n",
    "\n",
    "def run_eval(dataset):\n",
    "    \"\"\"Loads the dataset and calls run_test_case with each case\"\"\"\n",
    "    results = []\n",
    "\n",
    "    for test_case in dataset:\n",
    "        result = run_test_case(test_case)\n",
    "        results.append(result)\n",
    "\n",
    "    average_score = mean([result[\"score\"] for result in results])\n",
    "    print(f\"Average score: {average_score}\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30fae983",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T01:22:58.208830Z",
     "start_time": "2025-12-30T01:22:26.375849Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average score: 8.333333333333334\n"
     ]
    }
   ],
   "source": [
    "with open(\"dataset.json\", \"r\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "results = run_eval(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dbcc6111",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T01:22:58.252445Z",
     "start_time": "2025-12-30T01:22:58.227882Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"output\": \"\\n^(?!.*\\\\.\\\\.)(?!.*\\\\.\\\\-)(?!.*\\\\-\\\\.)[a-z0-9][a-z0-9.-]{1,61}[a-z0-9]$\\n\",\n",
      "    \"test_case\": {\n",
      "      \"task\": \"Write a regular expression that validates AWS S3 bucket names according to AWS naming rules: 3-63 characters long, only lowercase letters, numbers, hyphens, and periods, must start and end with a letter or number, and cannot have consecutive periods or period-hyphen combinations.\",\n",
      "      \"format\": \"regex\",\n",
      "      \"solution_criteria\": \"Regex must match valid S3 bucket names (e.g., 'my-bucket-123', 'example.bucket') and reject invalid ones (e.g., 'My-Bucket', 'bucket..name', '-invalid', 'ab'). Should enforce all AWS S3 naming constraints.\"\n",
      "    },\n",
      "    \"score\": 8.5,\n",
      "    \"reasoning\": \"The regex handles most basic S3 naming rules well, including character restrictions, start/end requirements, and preventing problematic character combinations. However, it has a subtle length issue and misses two important AWS-specific restrictions: IP address formats and the 'xn--' prefix prohibition. These are documented AWS constraints that would cause bucket creation to fail.\"\n",
      "  },\n",
      "  {\n",
      "    \"output\": \"\\n{\\n  \\\"Version\\\": \\\"2012-10-17\\\",\\n  \\\"Statement\\\": [\\n    {\\n      \\\"Effect\\\": \\\"Allow\\\",\\n      \\\"Action\\\": [\\n        \\\"s3:GetObject\\\",\\n        \\\"s3:ListBucket\\\"\\n      ],\\n      \\\"Resource\\\": [\\n        \\\"arn:aws:s3:::company-reports\\\",\\n        \\\"arn:aws:s3:::company-reports/*\\\"\\n      ]\\n    }\\n  ]\\n}\\n\",\n",
      "    \"test_case\": {\n",
      "      \"task\": \"Create a JSON object representing an AWS IAM policy that grants read-only access to a specific S3 bucket named 'company-reports' and all objects within it.\",\n",
      "      \"format\": \"json\",\n",
      "      \"solution_criteria\": \"JSON must be a valid IAM policy document with Version, Statement array, Effect set to 'Allow', appropriate Action(s) for read operations (s3:GetObject, s3:ListBucket), and Resource ARN(s) correctly formatted for the specified bucket.\"\n",
      "    },\n",
      "    \"score\": 9.0,\n",
      "    \"reasoning\": \"The policy correctly implements the core requirements for read-only S3 access with proper JSON structure, valid ARN formatting, and the two fundamental read actions. However, it represents a minimal implementation that may be insufficient for real-world scenarios involving versioned buckets or console access. For basic programmatic read access, this policy is functional and correct.\"\n",
      "  },\n",
      "  {\n",
      "    \"output\": \"\\ndef extract_service_name(arn):\\n    return arn.split(':')[2]\\n\",\n",
      "    \"test_case\": {\n",
      "      \"task\": \"Write a Python function that takes an AWS ARN string as input and extracts the service name from it. For example, given 'arn:aws:s3:::my-bucket/file.txt', it should return 's3'.\",\n",
      "      \"format\": \"python\",\n",
      "      \"solution_criteria\": \"Function must correctly parse standard AWS ARN format (arn:partition:service:region:account-id:resource) and return the service component. Should handle various ARN formats and return None or raise an exception for invalid ARNs.\"\n",
      "    },\n",
      "    \"score\": 7.5,\n",
      "    \"reasoning\": \"While the core logic correctly extracts the service name from valid ARNs, the solution lacks essential error handling and input validation required by the criteria. Production code should gracefully handle invalid inputs by returning None or raising descriptive exceptions rather than allowing IndexError to propagate. A robust solution would validate the ARN structure before extraction.\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(results, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
