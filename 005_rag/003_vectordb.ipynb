{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T05:14:20.772288Z",
     "start_time": "2025-12-31T05:14:20.745841Z"
    }
   },
   "outputs": [],
   "source": [
    "from os import environ\n",
    "\n",
    "# Client Setup\n",
    "from google import genai\n",
    "\n",
    "region = environ.get(\"CLOUD_ML_REGION\", \"\")\n",
    "project_id = environ.get(\"ANTHROPIC_VERTEX_PROJECT_ID\", \"\")\n",
    "client = genai.Client(project=project_id, location=region, vertexai=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T05:14:20.796169Z",
     "start_time": "2025-12-31T05:14:20.774646Z"
    }
   },
   "outputs": [],
   "source": [
    "# Chunk by section\n",
    "import re\n",
    "\n",
    "\n",
    "def chunk_by_section(document_text):\n",
    "    \"\"\"Chunk text based on Markdown sections (## headers)\"\"\"\n",
    "    pattern = r\"\\n## \"\n",
    "    return re.split(pattern, document_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T05:14:20.802419Z",
     "start_time": "2025-12-31T05:14:20.796710Z"
    }
   },
   "outputs": [],
   "source": [
    "# Embedding Generation\n",
    "def generate_embedding(\n",
    "    chunks: list[str] | str,\n",
    ") -> list[list[float]] | list[float]:\n",
    "    \"\"\"Generate embeddings for the given text using the specified model.\"\"\"\n",
    "    is_list = isinstance(chunks, list)\n",
    "    input_data = chunks if is_list else [chunks]\n",
    "\n",
    "    response = client.models.embed_content(\n",
    "        model=\"text-embedding-005\", contents=input_data\n",
    "    )\n",
    "\n",
    "    if not response.embeddings:\n",
    "        raise ValueError(\"API returned no embeddings\")\n",
    "\n",
    "    if len(response.embeddings) != len(input_data):\n",
    "        raise ValueError(\n",
    "            f\"Expected {len(input_data)} embeddings, got {len(response.embeddings)}\"\n",
    "        )\n",
    "\n",
    "    embeddings: list[list[float]] = []\n",
    "    for i, embedding in enumerate(response.embeddings):\n",
    "        if embedding.values is None:\n",
    "            raise ValueError(f\"Embedding {i} has no values\")\n",
    "        embeddings.append(embedding.values)\n",
    "\n",
    "    return embeddings if is_list else embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T05:14:20.859634Z",
     "start_time": "2025-12-31T05:14:20.802936Z"
    }
   },
   "outputs": [],
   "source": [
    "# VectorIndex implementation\n",
    "import math\n",
    "from typing import Optional, Any, List, Dict, Tuple\n",
    "\n",
    "\n",
    "class VectorIndex:\n",
    "    \"\"\"A simple in-memory vector index for storing and searching vectors.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        distance_metric: str = \"cosine\",\n",
    "        embedding_fn=None,\n",
    "    ):\n",
    "        self.vectors: List[List[float]] = []\n",
    "        self.documents: List[Dict[str, Any]] = []\n",
    "        self._vector_dim: Optional[int] = None\n",
    "        if distance_metric not in [\"cosine\", \"euclidean\"]:\n",
    "            raise ValueError(\"distance_metric must be 'cosine' or 'euclidean'\")\n",
    "        self._distance_metric = distance_metric\n",
    "        self._embedding_fn = embedding_fn\n",
    "\n",
    "    def add_document(self, document: Dict[str, Any]):\n",
    "        if not self._embedding_fn:\n",
    "            raise ValueError(\"Embedding function not provided during initialization.\")\n",
    "        if not isinstance(document, dict):\n",
    "            raise TypeError(\"Document must be a dictionary.\")\n",
    "        if \"content\" not in document:\n",
    "            raise ValueError(\"Document dictionary must contain a 'content' key.\")\n",
    "\n",
    "        content = document[\"content\"]\n",
    "        if not isinstance(content, str):\n",
    "            raise TypeError(\"Document 'content' must be a string.\")\n",
    "\n",
    "        vector = self._embedding_fn(content)\n",
    "        self.add_vector(vector=vector, document=document)\n",
    "\n",
    "    def search(self, query: Any, k: int = 1) -> List[Tuple[Dict[str, Any], float]]:\n",
    "        if not self.vectors:\n",
    "            return []\n",
    "\n",
    "        if isinstance(query, str):\n",
    "            if not self._embedding_fn:\n",
    "                raise ValueError(\"Embedding function not provided for string query.\")\n",
    "            query_vector = self._embedding_fn(query)\n",
    "        elif isinstance(query, list) and all(\n",
    "            isinstance(x, (int, float)) for x in query\n",
    "        ):\n",
    "            query_vector = query\n",
    "        else:\n",
    "            raise TypeError(\"Query must be either a string or a list of numbers.\")\n",
    "\n",
    "        if self._vector_dim is None:\n",
    "            return []\n",
    "\n",
    "        if len(query_vector) != self._vector_dim:\n",
    "            raise ValueError(\n",
    "                f\"Query vector dimension mismatch. Expected {self._vector_dim}, got {len(query_vector)}\"\n",
    "            )\n",
    "\n",
    "        if k <= 0:\n",
    "            raise ValueError(\"k must be a positive integer.\")\n",
    "\n",
    "        if self._distance_metric == \"cosine\":\n",
    "            dist_func = self._cosine_distance\n",
    "        else:\n",
    "            dist_func = self._euclidean_distance\n",
    "\n",
    "        distances = []\n",
    "        for i, stored_vector in enumerate(self.vectors):\n",
    "            distance = dist_func(query_vector, stored_vector)\n",
    "            distances.append((distance, self.documents[i]))\n",
    "\n",
    "        distances.sort(key=lambda item: item[0])\n",
    "\n",
    "        return [(doc, dist) for dist, doc in distances[:k]]\n",
    "\n",
    "    def add_vector(self, vector, document: Dict[str, Any]):\n",
    "        if not isinstance(vector, list) or not all(\n",
    "            isinstance(x, (int, float)) for x in vector\n",
    "        ):\n",
    "            raise TypeError(\"Vector must be a list of numbers.\")\n",
    "        if not isinstance(document, dict):\n",
    "            raise TypeError(\"Document must be a dictionary.\")\n",
    "        if \"content\" not in document:\n",
    "            raise ValueError(\"Document dictionary must contain a 'content' key.\")\n",
    "\n",
    "        if not self.vectors:\n",
    "            self._vector_dim = len(vector)\n",
    "        elif len(vector) != self._vector_dim:\n",
    "            raise ValueError(\n",
    "                f\"Inconsistent vector dimension. Expected {self._vector_dim}, got {len(vector)}\"\n",
    "            )\n",
    "\n",
    "        self.vectors.append(list(vector))\n",
    "        self.documents.append(document)\n",
    "\n",
    "    def _euclidean_distance(self, vec1: List[float], vec2: List[float]) -> float:\n",
    "        if len(vec1) != len(vec2):\n",
    "            raise ValueError(\"Vectors must have the same dimension\")\n",
    "        return math.sqrt(sum((p - q) ** 2 for p, q in zip(vec1, vec2)))\n",
    "\n",
    "    def _dot_product(self, vec1: List[float], vec2: List[float]) -> float:\n",
    "        if len(vec1) != len(vec2):\n",
    "            raise ValueError(\"Vectors must have the same dimension\")\n",
    "        return sum(p * q for p, q in zip(vec1, vec2))\n",
    "\n",
    "    def _magnitude(self, vec: list[float]) -> float:\n",
    "        return math.sqrt(sum(x * x for x in vec))\n",
    "\n",
    "    def _cosine_distance(self, vec1: list[float], vec2: list[float]) -> float:\n",
    "        if len(vec1) != len(vec2):\n",
    "            raise ValueError(\"Vectors must have the same dimension\")\n",
    "\n",
    "        mag1 = self._magnitude(vec1)\n",
    "        mag2 = self._magnitude(vec2)\n",
    "\n",
    "        if mag1 == 0 and mag2 == 0:\n",
    "            return 0.0\n",
    "        elif mag1 == 0 or mag2 == 0:\n",
    "            return 1.0\n",
    "\n",
    "        dot_prod = self._dot_product(vec1, vec2)\n",
    "        cosine_similarity = dot_prod / (mag1 * mag2)\n",
    "        cosine_similarity = max(-1.0, min(1.0, cosine_similarity))\n",
    "\n",
    "        return 1.0 - cosine_similarity\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.vectors)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        has_embed_fn = \"Yes\" if self._embedding_fn else \"No\"\n",
    "        return f\"VectorIndex(count={len(self)}, dim={self._vector_dim}, metric='{self._distance_metric}', has_embedding_fn='{has_embed_fn}')\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T05:14:20.864856Z",
     "start_time": "2025-12-31T05:14:20.860078Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"./report.md\", \"r\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T05:14:20.868833Z",
     "start_time": "2025-12-31T05:14:20.865307Z"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Chunk the text by section\n",
    "chunks = chunk_by_section(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T05:14:27.541325Z",
     "start_time": "2025-12-31T05:14:20.869160Z"
    }
   },
   "outputs": [],
   "source": [
    "# 2. Generate embeddings for each chunk\n",
    "embeddings = generate_embedding(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T05:14:27.562012Z",
     "start_time": "2025-12-31T05:14:27.552549Z"
    }
   },
   "outputs": [],
   "source": [
    "# 3. Create a vector store and add each embedding to it\n",
    "vector_store = VectorIndex(distance_metric=\"cosine\", embedding_fn=generate_embedding)\n",
    "for chunk, embedding in zip(chunks, embeddings):\n",
    "    document = {\"content\": chunk}\n",
    "    vector_store.add_vector(vector=embedding, document=document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T05:14:29.004661Z",
     "start_time": "2025-12-31T05:14:27.563314Z"
    }
   },
   "outputs": [],
   "source": [
    "# 4. Some time later, a user will ask a question. Generate an embedding for it\n",
    "user_question = \"What happened for INC-2023-Q4-011\"\n",
    "question_embedding = generate_embedding(user_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T05:14:29.023540Z",
     "start_time": "2025-12-31T05:14:29.016025Z"
    }
   },
   "outputs": [],
   "source": [
    "# 5. Search the store with the embedding, find the 2 most relevant chunks\n",
    "results = vector_store.search(query=question_embedding, k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T05:14:29.057750Z",
     "start_time": "2025-12-31T05:14:29.024300Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({'content': 'Section 2: Software Engineering - Project Phoenix Stability Enhancements\\n\\nThe Software Engineering division dedicated considerable effort to improving the stability and performance of the core systems underpinning Project Phoenix. Recurring issues, particularly `ERR_MEM_ALLOC_FAIL_0x8007000E` during peak loads and `TIMEOUT_QUERY_DB_0xDEADBEEF` affecting data retrieval operations, were prioritized, at a cost of INC-2023-Q4-011. Root cause analysis pointed towards inefficiencies in the primary data caching algorithm and suboptimal database indexing strategies. The deployment of a patch addressed the memory allocation error, resulting in a measured 40% reduction in critical failures under simulated stress tests during Q4 2024 (Test Case ID: INC-2023-Q4-011). Further refactoring of the query module, scheduled for the next release cycle, aims to resolve the timeout issue. These findings underscore the importance of robust testing protocols, especially given the dependencies identified by the Product Engineering team (Section 6). The team continues to monitor system telemetry closely for any regressions or newly emerging error patterns. During Q4 of 2024 the team also assisted with helping regarding the INC-2023-Q4-011 incident.\\n'},\n",
       "  0.3728740153350213),\n",
       " ({'content': \"Section 3: Financial Analysis - Q3 Performance and Outlook\\n\\nQuarterly financial analysis revealed a complex picture. Overall group revenue saw modest growth of 3.1% year-over-year, primarily driven by strong performance in the primary subsidiary's established markets. However, the emerging markets division experienced a slight contraction (-1.5%), attributed to increased competitive pressure and unfavorable currency fluctuations. Margin erosion was observed across several key product lines, linked to rising input costs and supply chain disruptions. Project Hercules, aimed at optimizing operational expenditures, yielded initial savings, but these were insufficient to fully offset the margin pressure. Investment in R&D initiatives, including those detailed in Section 9 (Pharmaceutical Development) and Section 4 (Scientific Experimentation), remained stable but faces potential review in light of these pressures. The team recommends a cautious outlook, emphasizing cost control and strategic resource allocation to protect core profitability while sustaining critical innovation pipelines. Further analysis is underway to model different investment scenarios.\\n\"},\n",
       "  0.40653923599549746)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
