{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T05:14:20.772288Z",
     "start_time": "2025-12-31T05:14:20.745841Z"
    },
    "execution": {
     "iopub.execute_input": "2026-01-09T02:45:28.410114Z",
     "iopub.status.busy": "2026-01-09T02:45:28.409953Z",
     "iopub.status.idle": "2026-01-09T02:45:29.688313Z",
     "shell.execute_reply": "2026-01-09T02:45:29.687731Z"
    }
   },
   "outputs": [],
   "source": [
    "from os import environ\n",
    "\n",
    "# Client Setup\n",
    "from google import genai\n",
    "\n",
    "region = environ.get(\"CLOUD_ML_REGION\", \"\")\n",
    "project_id = environ.get(\"ANTHROPIC_VERTEX_PROJECT_ID\", \"\")\n",
    "client = genai.Client(project=project_id, location=region, vertexai=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T05:14:20.796169Z",
     "start_time": "2025-12-31T05:14:20.774646Z"
    },
    "execution": {
     "iopub.execute_input": "2026-01-09T02:45:29.690633Z",
     "iopub.status.busy": "2026-01-09T02:45:29.690434Z",
     "iopub.status.idle": "2026-01-09T02:45:29.692798Z",
     "shell.execute_reply": "2026-01-09T02:45:29.692305Z"
    }
   },
   "outputs": [],
   "source": [
    "# Chunk by section\n",
    "import re\n",
    "\n",
    "\n",
    "def chunk_by_section(document_text):\n",
    "    \"\"\"Chunk text based on Markdown sections (## headers)\"\"\"\n",
    "    pattern = r\"\\n## \"\n",
    "    return re.split(pattern, document_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T05:14:20.802419Z",
     "start_time": "2025-12-31T05:14:20.796710Z"
    },
    "execution": {
     "iopub.execute_input": "2026-01-09T02:45:29.694709Z",
     "iopub.status.busy": "2026-01-09T02:45:29.694575Z",
     "iopub.status.idle": "2026-01-09T02:45:29.697933Z",
     "shell.execute_reply": "2026-01-09T02:45:29.697350Z"
    }
   },
   "outputs": [],
   "source": [
    "# Embedding Generation\n",
    "def generate_embedding(\n",
    "    chunks: list[str] | str,\n",
    ") -> list[list[float]] | list[float]:\n",
    "    \"\"\"Generate embeddings for the given text using the specified model.\"\"\"\n",
    "    is_list = isinstance(chunks, list)\n",
    "    input_data = chunks if is_list else [chunks]\n",
    "\n",
    "    response = client.models.embed_content(\n",
    "        model=\"text-embedding-005\", contents=input_data\n",
    "    )\n",
    "\n",
    "    if not response.embeddings:\n",
    "        raise ValueError(\"API returned no embeddings\")\n",
    "\n",
    "    if len(response.embeddings) != len(input_data):\n",
    "        raise ValueError(\n",
    "            f\"Expected {len(input_data)} embeddings, got {len(response.embeddings)}\"\n",
    "        )\n",
    "\n",
    "    embeddings: list[list[float]] = []\n",
    "    for i, embedding in enumerate(response.embeddings):\n",
    "        if embedding.values is None:\n",
    "            raise ValueError(f\"Embedding {i} has no values\")\n",
    "        embeddings.append(embedding.values)\n",
    "\n",
    "    return embeddings if is_list else embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T05:14:20.859634Z",
     "start_time": "2025-12-31T05:14:20.802936Z"
    },
    "execution": {
     "iopub.execute_input": "2026-01-09T02:45:29.699394Z",
     "iopub.status.busy": "2026-01-09T02:45:29.699306Z",
     "iopub.status.idle": "2026-01-09T02:45:29.708103Z",
     "shell.execute_reply": "2026-01-09T02:45:29.707465Z"
    }
   },
   "outputs": [],
   "source": [
    "# VectorIndex implementation\n",
    "import math\n",
    "from typing import Optional, Any, List, Dict, Tuple\n",
    "\n",
    "\n",
    "class VectorIndex:\n",
    "    \"\"\"A simple in-memory vector index for storing and searching vectors.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        distance_metric: str = \"cosine\",\n",
    "        embedding_fn=None,\n",
    "    ):\n",
    "        self.vectors: List[List[float]] = []\n",
    "        self.documents: List[Dict[str, Any]] = []\n",
    "        self._vector_dim: Optional[int] = None\n",
    "        if distance_metric not in [\"cosine\", \"euclidean\"]:\n",
    "            raise ValueError(\"distance_metric must be 'cosine' or 'euclidean'\")\n",
    "        self._distance_metric = distance_metric\n",
    "        self._embedding_fn = embedding_fn\n",
    "\n",
    "    def add_document(self, document: Dict[str, Any]):\n",
    "        if not self._embedding_fn:\n",
    "            raise ValueError(\"Embedding function not provided during initialization.\")\n",
    "        if not isinstance(document, dict):\n",
    "            raise TypeError(\"Document must be a dictionary.\")\n",
    "        if \"content\" not in document:\n",
    "            raise ValueError(\"Document dictionary must contain a 'content' key.\")\n",
    "\n",
    "        content = document[\"content\"]\n",
    "        if not isinstance(content, str):\n",
    "            raise TypeError(\"Document 'content' must be a string.\")\n",
    "\n",
    "        vector = self._embedding_fn(content)\n",
    "        self.add_vector(vector=vector, document=document)\n",
    "\n",
    "    def search(self, query: Any, k: int = 1) -> List[Tuple[Dict[str, Any], float]]:\n",
    "        if not self.vectors:\n",
    "            return []\n",
    "\n",
    "        if isinstance(query, str):\n",
    "            if not self._embedding_fn:\n",
    "                raise ValueError(\"Embedding function not provided for string query.\")\n",
    "            query_vector = self._embedding_fn(query)\n",
    "        elif isinstance(query, list) and all(\n",
    "            isinstance(x, (int, float)) for x in query\n",
    "        ):\n",
    "            query_vector = query\n",
    "        else:\n",
    "            raise TypeError(\"Query must be either a string or a list of numbers.\")\n",
    "\n",
    "        if self._vector_dim is None:\n",
    "            return []\n",
    "\n",
    "        if len(query_vector) != self._vector_dim:\n",
    "            raise ValueError(\n",
    "                f\"Query vector dimension mismatch. Expected {self._vector_dim}, got {len(query_vector)}\"\n",
    "            )\n",
    "\n",
    "        if k <= 0:\n",
    "            raise ValueError(\"k must be a positive integer.\")\n",
    "\n",
    "        if self._distance_metric == \"cosine\":\n",
    "            dist_func = self._cosine_distance\n",
    "        else:\n",
    "            dist_func = self._euclidean_distance\n",
    "\n",
    "        distances = []\n",
    "        for i, stored_vector in enumerate(self.vectors):\n",
    "            distance = dist_func(query_vector, stored_vector)\n",
    "            distances.append((distance, self.documents[i]))\n",
    "\n",
    "        distances.sort(key=lambda item: item[0])\n",
    "\n",
    "        return [(doc, dist) for dist, doc in distances[:k]]\n",
    "\n",
    "    def add_vector(self, vector, document: Dict[str, Any]):\n",
    "        if not isinstance(vector, list) or not all(\n",
    "            isinstance(x, (int, float)) for x in vector\n",
    "        ):\n",
    "            raise TypeError(\"Vector must be a list of numbers.\")\n",
    "        if not isinstance(document, dict):\n",
    "            raise TypeError(\"Document must be a dictionary.\")\n",
    "        if \"content\" not in document:\n",
    "            raise ValueError(\"Document dictionary must contain a 'content' key.\")\n",
    "\n",
    "        if not self.vectors:\n",
    "            self._vector_dim = len(vector)\n",
    "        elif len(vector) != self._vector_dim:\n",
    "            raise ValueError(\n",
    "                f\"Inconsistent vector dimension. Expected {self._vector_dim}, got {len(vector)}\"\n",
    "            )\n",
    "\n",
    "        self.vectors.append(list(vector))\n",
    "        self.documents.append(document)\n",
    "\n",
    "    def _euclidean_distance(self, vec1: List[float], vec2: List[float]) -> float:\n",
    "        if len(vec1) != len(vec2):\n",
    "            raise ValueError(\"Vectors must have the same dimension\")\n",
    "        return math.sqrt(sum((p - q) ** 2 for p, q in zip(vec1, vec2)))\n",
    "\n",
    "    def _dot_product(self, vec1: List[float], vec2: List[float]) -> float:\n",
    "        if len(vec1) != len(vec2):\n",
    "            raise ValueError(\"Vectors must have the same dimension\")\n",
    "        return sum(p * q for p, q in zip(vec1, vec2))\n",
    "\n",
    "    def _magnitude(self, vec: list[float]) -> float:\n",
    "        return math.sqrt(sum(x * x for x in vec))\n",
    "\n",
    "    def _cosine_distance(self, vec1: list[float], vec2: list[float]) -> float:\n",
    "        if len(vec1) != len(vec2):\n",
    "            raise ValueError(\"Vectors must have the same dimension\")\n",
    "\n",
    "        mag1 = self._magnitude(vec1)\n",
    "        mag2 = self._magnitude(vec2)\n",
    "\n",
    "        if mag1 == 0 and mag2 == 0:\n",
    "            return 0.0\n",
    "        elif mag1 == 0 or mag2 == 0:\n",
    "            return 1.0\n",
    "\n",
    "        dot_prod = self._dot_product(vec1, vec2)\n",
    "        cosine_similarity = dot_prod / (mag1 * mag2)\n",
    "        cosine_similarity = max(-1.0, min(1.0, cosine_similarity))\n",
    "\n",
    "        return 1.0 - cosine_similarity\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.vectors)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        has_embed_fn = \"Yes\" if self._embedding_fn else \"No\"\n",
    "        return f\"VectorIndex(count={len(self)}, dim={self._vector_dim}, metric='{self._distance_metric}', has_embedding_fn='{has_embed_fn}')\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T05:14:20.864856Z",
     "start_time": "2025-12-31T05:14:20.860078Z"
    },
    "execution": {
     "iopub.execute_input": "2026-01-09T02:45:29.709899Z",
     "iopub.status.busy": "2026-01-09T02:45:29.709763Z",
     "iopub.status.idle": "2026-01-09T02:45:29.712260Z",
     "shell.execute_reply": "2026-01-09T02:45:29.711919Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"./report.md\", \"r\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T05:14:20.868833Z",
     "start_time": "2025-12-31T05:14:20.865307Z"
    },
    "execution": {
     "iopub.execute_input": "2026-01-09T02:45:29.713842Z",
     "iopub.status.busy": "2026-01-09T02:45:29.713723Z",
     "iopub.status.idle": "2026-01-09T02:45:29.716058Z",
     "shell.execute_reply": "2026-01-09T02:45:29.715596Z"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Chunk the text by section\n",
    "chunks = chunk_by_section(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T05:14:27.541325Z",
     "start_time": "2025-12-31T05:14:20.869160Z"
    },
    "execution": {
     "iopub.execute_input": "2026-01-09T02:45:29.717535Z",
     "iopub.status.busy": "2026-01-09T02:45:29.717431Z",
     "iopub.status.idle": "2026-01-09T02:45:33.249817Z",
     "shell.execute_reply": "2026-01-09T02:45:33.248967Z"
    }
   },
   "outputs": [],
   "source": [
    "# 2. Generate embeddings for each chunk\n",
    "embeddings = generate_embedding(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T05:14:27.562012Z",
     "start_time": "2025-12-31T05:14:27.552549Z"
    },
    "execution": {
     "iopub.execute_input": "2026-01-09T02:45:33.252583Z",
     "iopub.status.busy": "2026-01-09T02:45:33.252355Z",
     "iopub.status.idle": "2026-01-09T02:45:33.256467Z",
     "shell.execute_reply": "2026-01-09T02:45:33.255733Z"
    }
   },
   "outputs": [],
   "source": [
    "# 3. Create a vector store and add each embedding to it\n",
    "vector_store = VectorIndex(distance_metric=\"cosine\", embedding_fn=generate_embedding)\n",
    "for chunk, embedding in zip(chunks, embeddings):\n",
    "    document = {\"content\": chunk}\n",
    "    vector_store.add_vector(vector=embedding, document=document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T05:14:29.004661Z",
     "start_time": "2025-12-31T05:14:27.563314Z"
    },
    "execution": {
     "iopub.execute_input": "2026-01-09T02:45:33.258330Z",
     "iopub.status.busy": "2026-01-09T02:45:33.258209Z",
     "iopub.status.idle": "2026-01-09T02:45:34.735666Z",
     "shell.execute_reply": "2026-01-09T02:45:34.734657Z"
    }
   },
   "outputs": [],
   "source": [
    "# 4. Some time later, a user will ask a question. Generate an embedding for it\n",
    "user_question = \"What happened for INC-2023-Q4-011\"\n",
    "question_embedding = generate_embedding(user_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T05:14:29.023540Z",
     "start_time": "2025-12-31T05:14:29.016025Z"
    },
    "execution": {
     "iopub.execute_input": "2026-01-09T02:45:34.737778Z",
     "iopub.status.busy": "2026-01-09T02:45:34.737639Z",
     "iopub.status.idle": "2026-01-09T02:45:34.740992Z",
     "shell.execute_reply": "2026-01-09T02:45:34.740293Z"
    }
   },
   "outputs": [],
   "source": [
    "# 5. Search the store with the embedding, find the 2 most relevant chunks\n",
    "results = vector_store.search(query=question_embedding, k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T05:14:29.057750Z",
     "start_time": "2025-12-31T05:14:29.024300Z"
    },
    "execution": {
     "iopub.execute_input": "2026-01-09T02:45:34.743092Z",
     "iopub.status.busy": "2026-01-09T02:45:34.742903Z",
     "iopub.status.idle": "2026-01-09T02:45:34.747183Z",
     "shell.execute_reply": "2026-01-09T02:45:34.746549Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({'content': 'Section 2: Software Engineering - Project Phoenix Stability Enhancements\\n\\nThe Software Engineering division dedicated considerable effort to improving the stability and performance of the core systems underpinning Project Phoenix. Recurring issues, particularly `ERR_MEM_ALLOC_FAIL_0x8007000E` during peak loads and `TIMEOUT_QUERY_DB_0xDEADBEEF` affecting data retrieval operations, were prioritized, at a cost of INC-2023-Q4-011. Root cause analysis pointed towards inefficiencies in the primary data caching algorithm and suboptimal database indexing strategies. The deployment of a patch addressed the memory allocation error, resulting in a measured 40% reduction in critical failures under simulated stress tests during Q4 2024 (Test Case ID: INC-2023-Q4-011). Further refactoring of the query module, scheduled for the next release cycle, aims to resolve the timeout issue. These findings underscore the importance of robust testing protocols, especially given the dependencies identified by the Product Engineering team (Section 6). The team continues to monitor system telemetry closely for any regressions or newly emerging error patterns. During Q4 of 2024 the team also assisted with helping regarding the INC-2023-Q4-011 incident.\\n'},\n",
       "  0.367089042395784),\n",
       " ({'content': 'Section 10: Cybersecurity Analysis - Incident Response Report: INC-2023-Q4-011\\n\\nThe Cybersecurity Operations Center successfully contained and remediated a targeted intrusion attempt tracked as `INC-2023-Q4-011`. Threat intelligence indicates the activity aligns with tactics, techniques, and procedures associated with the `ShadowNet Syndicate` threat actor group. Initial access was gained via a spear-phishing email targeting personnel within the finance department, potentially seeking data relevant to Section 3 (Financial Analysis). Endpoint detection and response (EDR) systems flagged anomalous process execution (`PID: 7812`) on workstation `WS-FIN-112`. Subsequent investigation identified malware (`SHA256:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855`) attempting lateral movement towards server `SRV-FIN-03`. Containment involved isolating affected systems and blocking associated command-and-control infrastructure (IP `198.51.100.24`). Mitigation included deploying updated endpoint policies and implementing enhanced perimeter filtering (`Firewall Rule ID: FN7832`). This incident highlights the persistent threat landscape and the need for ongoing vigilance and user training, particularly concerning sensitive financial and potentially research data (e.g., Section 1, Section 9). Forensics analysis is ongoing.\\n'},\n",
       "  0.420961916392979)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
