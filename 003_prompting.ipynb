{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T06:22:17.024336Z",
     "start_time": "2025-12-30T06:22:14.520493Z"
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import json\n",
    "import re\n",
    "from textwrap import dedent\n",
    "from statistics import mean\n",
    "from anthropic import AnthropicVertex\n",
    "from os import environ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T06:22:17.261470Z",
     "start_time": "2025-12-30T06:22:17.026845Z"
    }
   },
   "outputs": [],
   "source": [
    "# Client Initialization and helper functions\n",
    "\n",
    "region = environ.get(\"CLOUD_ML_REGION\", \"\")\n",
    "project_id = environ.get(\"ANTHROPIC_VERTEX_PROJECT_ID\", \"\")\n",
    "client = AnthropicVertex(region=region, project_id=project_id)\n",
    "model = \"claude-sonnet-4-5@20250929\"\n",
    "\n",
    "\n",
    "def add_user_message(messages, text):\n",
    "    user_message = {\"role\": \"user\", \"content\": text}\n",
    "    messages.append(user_message)\n",
    "\n",
    "\n",
    "def add_assistant_message(messages, text):\n",
    "    assistant_message = {\"role\": \"assistant\", \"content\": text}\n",
    "    messages.append(assistant_message)\n",
    "\n",
    "\n",
    "def chat(messages, system=None, temperature=1.0, stop_sequences=[]):\n",
    "    params = {\n",
    "        \"model\": model,\n",
    "        \"max_tokens\": 1000,\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": temperature,\n",
    "        \"stop_sequences\": stop_sequences,\n",
    "    }\n",
    "\n",
    "    if system:\n",
    "        params[\"system\"] = system\n",
    "\n",
    "    message = client.messages.create(**params)\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T06:22:17.271156Z",
     "start_time": "2025-12-30T06:22:17.262447Z"
    }
   },
   "outputs": [],
   "source": [
    "# Report Builder\n",
    "def generate_prompt_evaluation_report(evaluation_results):\n",
    "    total_tests = len(evaluation_results)\n",
    "    scores = [result[\"score\"] for result in evaluation_results]\n",
    "    avg_score = mean(scores) if scores else 0\n",
    "    max_possible_score = 10\n",
    "    pass_rate = (\n",
    "        100 * len([s for s in scores if s >= 7]) / total_tests if total_tests else 0\n",
    "    )\n",
    "\n",
    "    html = f\"\"\"\n",
    "    <!DOCTYPE html>\n",
    "    <html lang=\"en\">\n",
    "    <head>\n",
    "        <meta charset=\"UTF-8\">\n",
    "        <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "        <title>Prompt Evaluation Report</title>\n",
    "        <style>\n",
    "            body {{\n",
    "                font-family: Arial, sans-serif;\n",
    "                line-height: 1.6;\n",
    "                margin: 0;\n",
    "                padding: 20px;\n",
    "                color: #333;\n",
    "            }}\n",
    "            .header {{\n",
    "                background-color: #f0f0f0;\n",
    "                padding: 20px;\n",
    "                border-radius: 5px;\n",
    "                margin-bottom: 20px;\n",
    "            }}\n",
    "            .summary-stats {{\n",
    "                display: flex;\n",
    "                justify-content: space-between;\n",
    "                flex-wrap: wrap;\n",
    "                gap: 10px;\n",
    "            }}\n",
    "            .stat-box {{\n",
    "                background-color: #fff;\n",
    "                border-radius: 5px;\n",
    "                padding: 15px;\n",
    "                box-shadow: 0 2px 5px rgba(0,0,0,0.1);\n",
    "                flex-basis: 30%;\n",
    "                min-width: 200px;\n",
    "            }}\n",
    "            .stat-value {{\n",
    "                font-size: 24px;\n",
    "                font-weight: bold;\n",
    "                margin-top: 5px;\n",
    "            }}\n",
    "            table {{\n",
    "                width: 100%;\n",
    "                border-collapse: collapse;\n",
    "                margin-top: 20px;\n",
    "            }}\n",
    "            th {{\n",
    "                background-color: #4a4a4a;\n",
    "                color: white;\n",
    "                text-align: left;\n",
    "                padding: 12px;\n",
    "            }}\n",
    "            td {{\n",
    "                padding: 10px;\n",
    "                border-bottom: 1px solid #ddd;\n",
    "                vertical-align: top;\n",
    "            }}\n",
    "            tr:nth-child(even) {{\n",
    "                background-color: #f9f9f9;\n",
    "            }}\n",
    "            .output-cell {{\n",
    "                white-space: pre-wrap;\n",
    "            }}\n",
    "            .score {{\n",
    "                font-weight: bold;\n",
    "                padding: 5px 10px;\n",
    "                border-radius: 3px;\n",
    "                display: inline-block;\n",
    "            }}\n",
    "            .score-high {{\n",
    "                background-color: #c8e6c9;\n",
    "                color: #2e7d32;\n",
    "            }}\n",
    "            .score-medium {{\n",
    "                background-color: #fff9c4;\n",
    "                color: #f57f17;\n",
    "            }}\n",
    "            .score-low {{\n",
    "                background-color: #ffcdd2;\n",
    "                color: #c62828;\n",
    "            }}\n",
    "            .output {{\n",
    "                overflow: auto;\n",
    "                white-space: pre-wrap;\n",
    "            }}\n",
    "\n",
    "            .output pre {{\n",
    "                background-color: #f5f5f5;\n",
    "                border: 1px solid #ddd;\n",
    "                border-radius: 4px;\n",
    "                padding: 10px;\n",
    "                margin: 0;\n",
    "                font-family: 'Consolas', 'Monaco', 'Courier New', monospace;\n",
    "                font-size: 14px;\n",
    "                line-height: 1.4;\n",
    "                color: #333;\n",
    "                box-shadow: inset 0 1px 3px rgba(0, 0, 0, 0.1);\n",
    "                overflow-x: auto;\n",
    "                white-space: pre-wrap; \n",
    "                word-wrap: break-word; \n",
    "            }}\n",
    "\n",
    "            td {{\n",
    "                width: 20%;\n",
    "            }}\n",
    "            .score-col {{\n",
    "                width: 80px;\n",
    "            }}\n",
    "        </style>\n",
    "    </head>\n",
    "    <body>\n",
    "        <div class=\"header\">\n",
    "            <h1>Prompt Evaluation Report</h1>\n",
    "            <div class=\"summary-stats\">\n",
    "                <div class=\"stat-box\">\n",
    "                    <div>Total Test Cases</div>\n",
    "                    <div class=\"stat-value\">{total_tests}</div>\n",
    "                </div>\n",
    "                <div class=\"stat-box\">\n",
    "                    <div>Average Score</div>\n",
    "                    <div class=\"stat-value\">{avg_score:.1f} / {max_possible_score}</div>\n",
    "                </div>\n",
    "                <div class=\"stat-box\">\n",
    "                    <div>Pass Rate (≥7)</div>\n",
    "                    <div class=\"stat-value\">{pass_rate:.1f}%</div>\n",
    "                </div>\n",
    "            </div>\n",
    "        </div>\n",
    "\n",
    "        <table>\n",
    "            <thead>\n",
    "                <tr>\n",
    "                    <th>Scenario</th>\n",
    "                    <th>Prompt Inputs</th>\n",
    "                    <th>Solution Criteria</th>\n",
    "                    <th>Output</th>\n",
    "                    <th>Score</th>\n",
    "                    <th>Reasoning</th>\n",
    "                </tr>\n",
    "            </thead>\n",
    "            <tbody>\n",
    "    \"\"\"\n",
    "\n",
    "    for result in evaluation_results:\n",
    "        prompt_inputs_html = \"<br>\".join(\n",
    "            [\n",
    "                f\"<strong>{key}:</strong> {value}\"\n",
    "                for key, value in result[\"test_case\"][\"prompt_inputs\"].items()\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        criteria_string = \"<br>• \".join(result[\"test_case\"][\"solution_criteria\"])\n",
    "\n",
    "        score = result[\"score\"]\n",
    "        if score >= 8:\n",
    "            score_class = \"score-high\"\n",
    "        elif score <= 5:\n",
    "            score_class = \"score-low\"\n",
    "        else:\n",
    "            score_class = \"score-medium\"\n",
    "\n",
    "        html += f\"\"\"\n",
    "            <tr>\n",
    "                <td>{result[\"test_case\"][\"scenario\"]}</td>\n",
    "                <td class=\"prompt-inputs\">{prompt_inputs_html}</td>\n",
    "                <td class=\"criteria\">• {criteria_string}</td>\n",
    "                <td class=\"output\"><pre>{result[\"output\"]}</pre></td>\n",
    "                <td class=\"score-col\"><span class=\"score {score_class}\">{score}</span></td>\n",
    "                <td class=\"reasoning\">{result[\"reasoning\"]}</td>\n",
    "            </tr>\n",
    "        \"\"\"\n",
    "\n",
    "    html += \"\"\"\n",
    "            </tbody>\n",
    "        </table>\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "\n",
    "    return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T06:23:07.618734Z",
     "start_time": "2025-12-30T06:23:07.599737Z"
    }
   },
   "outputs": [],
   "source": [
    "from concurrent import futures\n",
    "\n",
    "\n",
    "# PromptEvaluator Implementation\n",
    "class PromptEvaluator:\n",
    "    def __init__(self, max_concurrent_tasks=3):\n",
    "        self.max_concurrent_tasks = max_concurrent_tasks\n",
    "\n",
    "    def render(self, template_string, variables):\n",
    "        placeholders = re.findall(r\"{([^{}]+)}\", template_string)\n",
    "\n",
    "        result = template_string\n",
    "        for placeholder in placeholders:\n",
    "            if placeholder in variables:\n",
    "                result = result.replace(\n",
    "                    \"{\" + placeholder + \"}\", str(variables[placeholder])\n",
    "                )\n",
    "\n",
    "        return result.replace(\"{{\", \"{\").replace(\"}}\", \"}\")\n",
    "\n",
    "    def generate_unique_ideas(self, task_description, prompt_inputs_spec, num_cases):\n",
    "        \"\"\"Generate a list of unique ideas for test cases based on the task description\"\"\"\n",
    "\n",
    "        prompt = \"\"\"\n",
    "        Generate {num_cases} unique, diverse ideas for testing a prompt that accomplishes this task:\n",
    "        \n",
    "        <task_description>\n",
    "        {task_description}\n",
    "        </task_description>\n",
    "\n",
    "        The prompt will receive the following inputs\n",
    "        <prompt_inputs>\n",
    "        {prompt_inputs_spec}\n",
    "        </prompt_inputs>\n",
    "        \n",
    "        Each idea should represent a distinct scenario or example that tests different aspects of the task.\n",
    "        \n",
    "        Output Format:\n",
    "        Provide your response as a structured JSON array where each item is a brief description of the idea.\n",
    "        \n",
    "        Example:\n",
    "        ```json\n",
    "        [\n",
    "            \"Testing with technical computer science terminology\",\n",
    "            \"Testing with medical research findings\",\n",
    "            \"Testing with complex mathematical concepts\",\n",
    "            ...\n",
    "        ]\n",
    "        ```\n",
    "        \n",
    "        Ensure each idea is:\n",
    "        - Clearly distinct from the others\n",
    "        - Relevant to the task description\n",
    "        - Specific enough to guide generation of a full test case\n",
    "        - Quick to solve without requiring extensive computation or multi-step processing\n",
    "        - Solvable with no more than 400 tokens of output\n",
    "\n",
    "        Remember, only generate {num_cases} unique ideas\n",
    "        \"\"\"\n",
    "\n",
    "        system_prompt = \"You are a test scenario designer specialized in creating diverse, unique testing scenarios.\"\n",
    "\n",
    "        example_prompt_inputs = \"\"\n",
    "        for key, value in prompt_inputs_spec.items():\n",
    "            val = value.replace(\"\\n\", \"\\\\n\")\n",
    "            example_prompt_inputs += f'\"{key}\": str # {val},'\n",
    "\n",
    "        rendered_prompt = self.render(\n",
    "            dedent(prompt),\n",
    "            {\n",
    "                \"task_description\": task_description,\n",
    "                \"num_cases\": num_cases,\n",
    "                \"prompt_inputs\": example_prompt_inputs,\n",
    "            },\n",
    "        )\n",
    "\n",
    "        messages = []\n",
    "        add_user_message(messages, rendered_prompt)\n",
    "        add_assistant_message(messages, \"```json\")\n",
    "        text = chat(\n",
    "            messages,\n",
    "            stop_sequences=[\"```\"],\n",
    "            system=system_prompt,\n",
    "            temperature=1.0,\n",
    "        )\n",
    "\n",
    "        return json.loads(text)\n",
    "\n",
    "    def generate_test_case(self, task_description, idea, prompt_inputs_spec={}):\n",
    "        \"\"\"Generate a single test case based on the task description and a specific idea\"\"\"\n",
    "\n",
    "        example_prompt_inputs = \"\"\n",
    "        for key, value in prompt_inputs_spec.items():\n",
    "            val = value.replace(\"\\n\", \"\\\\n\")\n",
    "            example_prompt_inputs += f'\"{key}\": \"EXAMPLE_VALUE\", // {val}\\n'\n",
    "\n",
    "        allowed_keys = \", \".join([f'\"{key}\"' for key in prompt_inputs_spec.keys()])\n",
    "\n",
    "        prompt = \"\"\"\n",
    "        Generate a single detailed test case for a prompt evaluation based on:\n",
    "        \n",
    "        <task_description>\n",
    "        {task_description}\n",
    "        </task_description>\n",
    "        \n",
    "        <specific_idea>\n",
    "        {idea}\n",
    "        </specific_idea>\n",
    "        \n",
    "        <allowed_input_keys>\n",
    "        {allowed_keys}\n",
    "        </allowed_input_keys>\n",
    "        \n",
    "        Output Format:\n",
    "        ```json\n",
    "        {{\n",
    "            \"prompt_inputs\": {{\n",
    "            {example_prompt_inputs}\n",
    "            }},\n",
    "            \"solution_criteria\": [\"criterion 1\", \"criterion 2\", ...] // Concise list of criteria for evaluating the solution, 1 to 4 items\n",
    "        }}\n",
    "        ```\n",
    "        \n",
    "        IMPORTANT REQUIREMENTS:\n",
    "        - You MUST ONLY use these exact input keys in your prompt_inputs: {allowed_keys}        \n",
    "        - Do NOT add any additional keys to prompt_inputs\n",
    "        - All keys listed in allowed_input_keys must be included in your response\n",
    "        - Make the test case realistic and practically useful\n",
    "        - Include measurable, concise solution criteria\n",
    "        - The solution criteria should ONLY address the direct requirements of the task description and the generated prompt_inputs\n",
    "        - Avoid over-specifying criteria with requirements that go beyond the core task\n",
    "        - Keep solution criteria simple, focused, and directly tied to the fundamental task\n",
    "        - The test case should be tailored to the specific idea provided\n",
    "        - Quick to solve without requiring extensive computation or multi-step processing\n",
    "        - Solvable with no more than 400 tokens of output\n",
    "        - DO NOT include any fields beyond those specified in the output format\n",
    "\n",
    "        Here's an example of a sample input with an ideal output:\n",
    "        <sample_input>\n",
    "        <sample_task_description>\n",
    "        Extract topics out of a passage of text\n",
    "        </sample_task_description>\n",
    "        <sample_specific_idea>\n",
    "        Testing with a text that contains multiple nested topics and subtopics (e.g., a passage about renewable energy that covers solar power economics, wind turbine technology, and policy implications simultaneously)\n",
    "        </sample_specific_idea>\n",
    "\n",
    "        <sample_allowed_input_keys>\n",
    "        \"content\"\n",
    "        </sample_allowed_input_keys>\n",
    "        </sample_input>\n",
    "        <ideal_output>\n",
    "        ```json\n",
    "        {\n",
    "            \"prompt_inputs\": {\n",
    "                \"content\": \"The transition to renewable energy encompasses numerous interdependent dimensions. Solar photovoltaic technology has seen dramatic cost reductions, with panel efficiency improving 24% since 2010 while manufacturing costs declined by 89%, making it economically competitive with fossil fuels in many markets. Concurrently, wind energy has evolved through innovative turbine designs featuring carbon-fiber composite blades and advanced control systems that increase energy capture by 35% in low-wind conditions.\"\n",
    "            },\n",
    "            \"solution_criteria\": [\n",
    "                \"Includes all topics mentioned\"   \n",
    "            ]\n",
    "        }\n",
    "        ```\n",
    "        </ideal_output>\n",
    "        This is ideal output because the solution criteria is concise and doesn't ask for anything outside of the scope of the task description.\n",
    "        \"\"\"\n",
    "\n",
    "        system_prompt = \"You are a test case creator specializing in designing evaluation scenarios.\"\n",
    "\n",
    "        rendered_prompt = self.render(\n",
    "            dedent(prompt),\n",
    "            {\n",
    "                \"allowed_keys\": allowed_keys,\n",
    "                \"task_description\": task_description,\n",
    "                \"idea\": idea,\n",
    "                \"example_prompt_inputs\": example_prompt_inputs,\n",
    "            },\n",
    "        )\n",
    "\n",
    "        messages = []\n",
    "        add_user_message(messages, rendered_prompt)\n",
    "        add_assistant_message(messages, \"```json\")\n",
    "        text = chat(\n",
    "            messages,\n",
    "            stop_sequences=[\"```\"],\n",
    "            system=system_prompt,\n",
    "            temperature=0.7,\n",
    "        )\n",
    "\n",
    "        test_case = json.loads(text)\n",
    "        test_case[\"task_description\"] = task_description\n",
    "        test_case[\"scenario\"] = idea\n",
    "\n",
    "        return test_case\n",
    "\n",
    "    def generate_dataset(\n",
    "        self,\n",
    "        task_description,\n",
    "            prompt_inputs_spec=None,\n",
    "        num_cases=1,\n",
    "        output_file=\"dataset.json\",\n",
    "    ):\n",
    "        \"\"\"Generate test dataset based on task description and save to file\"\"\"\n",
    "        if prompt_inputs_spec is None:\n",
    "            prompt_inputs_spec = {}\n",
    "        ideas = self.generate_unique_ideas(\n",
    "            task_description, prompt_inputs_spec, num_cases\n",
    "        )\n",
    "\n",
    "        dataset = []\n",
    "        completed = 0\n",
    "        total = len(ideas)\n",
    "        last_reported_percentage = 0\n",
    "\n",
    "        with futures.ThreadPoolExecutor(\n",
    "            max_workers=self.max_concurrent_tasks\n",
    "        ) as executor:\n",
    "            future_to_idea = {\n",
    "                executor.submit(\n",
    "                    self.generate_test_case,\n",
    "                    task_description,\n",
    "                    idea,\n",
    "                    prompt_inputs_spec,\n",
    "                ): idea\n",
    "                for idea in ideas\n",
    "            }\n",
    "\n",
    "            for future in futures.as_completed(future_to_idea):\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    completed += 1\n",
    "                    current_percentage = int((completed / total) * 100)\n",
    "                    milestone_percentage = (current_percentage // 20) * 20\n",
    "\n",
    "                    if milestone_percentage > last_reported_percentage:\n",
    "                        print(f\"Generated {completed}/{total} test cases\")\n",
    "                        last_reported_percentage = milestone_percentage\n",
    "\n",
    "                    dataset.append(result)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error generating test case: {e}\")\n",
    "\n",
    "        with open(output_file, \"w\") as f:\n",
    "            json.dump(dataset, f, indent=2)\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    def grade_output(self, test_case, output, extra_criteria):\n",
    "        \"\"\"Grade the output of a test case using the model\"\"\"\n",
    "\n",
    "        prompt_inputs = \"\"\n",
    "        for key, value in test_case[\"prompt_inputs\"].items():\n",
    "            val = value.replace(\"\\n\", \"\\\\n\")\n",
    "            prompt_inputs += f'\"{key}\":\"{val}\",\\n'\n",
    "\n",
    "        extra_criteria_section = \"\"\n",
    "        if extra_criteria:\n",
    "            extra_criteria_template = \"\"\"\n",
    "            Mandatory Requirements - ANY VIOLATION MEANS AUTOMATIC FAILURE (score of 3 or lower):\n",
    "            <extra_important_criteria>\n",
    "            {extra_criteria}\n",
    "            </extra_important_criteria>\n",
    "            \"\"\"\n",
    "            extra_criteria_section = self.render(\n",
    "                dedent(extra_criteria_template),\n",
    "                {\"extra_criteria\": extra_criteria},\n",
    "            )\n",
    "\n",
    "        eval_template = \"\"\"\n",
    "        Your task is to evaluate the following AI-generated solution with EXTREME RIGOR.\n",
    "\n",
    "        Original task description:\n",
    "        <task_description>\n",
    "        {task_description}\n",
    "        </task_description>\n",
    "\n",
    "        Original task inputs:\n",
    "        <task_inputs>\n",
    "        {{ {prompt_inputs} }}\n",
    "        </task_inputs>\n",
    "\n",
    "        Solution to Evaluate:\n",
    "        <solution>\n",
    "        {output}\n",
    "        </solution>\n",
    "\n",
    "        Criteria you should use to evaluate the solution:\n",
    "        <criteria>\n",
    "        {solution_criteria}\n",
    "        </criteria>\n",
    "\n",
    "        {extra_criteria_section}\n",
    "\n",
    "        Scoring Guidelines:\n",
    "        * Score 1-3: Solution fails to meet one or more MANDATORY requirements\n",
    "        * Score 4-6: Solution meets all mandatory requirements but has significant deficiencies in secondary criteria\n",
    "        * Score 7-8: Solution meets all mandatory requirements and most secondary criteria, with minor issues\n",
    "        * Score 9-10: Solution meets all mandatory and secondary criteria\n",
    "\n",
    "        IMPORTANT SCORING INSTRUCTIONS:\n",
    "        * Grade the output based ONLY on the listed criteria. Do not add your own extra requirements.\n",
    "        * If a solution meets all of the mandatory and secondary criteria give it a 10\n",
    "        * Don't complain that the solution \"only\" meets the mandatory and secondary criteria. Solutions shouldn't go above and beyond - they should meet the exact listed criteria.\n",
    "        * ANY violation of a mandatory requirement MUST result in a score of 3 or lower\n",
    "        * The full 1-10 scale should be utilized - don't hesitate to give low scores when warranted\n",
    "\n",
    "        Output Format\n",
    "        Provide your evaluation as a structured JSON object with the following fields, in this specific order:\n",
    "        - \"strengths\": An array of 1-3 key strengths\n",
    "        - \"weaknesses\": An array of 1-3 key areas for improvement\n",
    "        - \"reasoning\": A concise explanation of your overall assessment\n",
    "        - \"score\": A number between 1-10\n",
    "\n",
    "        Respond with JSON. Keep your response concise and direct.\n",
    "        Example response shape:\n",
    "        {{\n",
    "            \"strengths\": string[],\n",
    "            \"weaknesses\": string[],\n",
    "            \"reasoning\": string,\n",
    "            \"score\": number\n",
    "        }}\n",
    "        \"\"\"\n",
    "\n",
    "        eval_prompt = self.render(\n",
    "            dedent(eval_template),\n",
    "            {\n",
    "                \"task_description\": test_case[\"task_description\"],\n",
    "                \"prompt_inputs\": prompt_inputs,\n",
    "                \"output\": output,\n",
    "                \"solution_criteria\": \"\\n\".join(test_case[\"solution_criteria\"]),\n",
    "                \"extra_criteria_section\": extra_criteria_section,\n",
    "            },\n",
    "        )\n",
    "\n",
    "        messages = []\n",
    "        add_user_message(messages, eval_prompt)\n",
    "        add_assistant_message(messages, \"```json\")\n",
    "        eval_text = chat(\n",
    "            messages,\n",
    "            stop_sequences=[\"```\"],\n",
    "            temperature=0.0,\n",
    "        )\n",
    "        return json.loads(eval_text)\n",
    "\n",
    "    def run_test_case(self, test_case, run_prompt_function, extra_criteria=None):\n",
    "        \"\"\"Run a test case and grade the result\"\"\"\n",
    "        output = run_prompt_function(test_case[\"prompt_inputs\"])\n",
    "\n",
    "        model_grade = self.grade_output(test_case, output, extra_criteria)\n",
    "        model_score = model_grade[\"score\"]\n",
    "        reasoning = model_grade[\"reasoning\"]\n",
    "\n",
    "        return {\n",
    "            \"output\": output,\n",
    "            \"test_case\": test_case,\n",
    "            \"score\": model_score,\n",
    "            \"reasoning\": reasoning,\n",
    "        }\n",
    "\n",
    "    def run_evaluation(\n",
    "        self,\n",
    "        run_prompt_function,\n",
    "        dataset_file,\n",
    "        extra_criteria=None,\n",
    "        json_output_file=\"output.json\",\n",
    "        html_output_file=\"output.html\",\n",
    "    ):\n",
    "        \"\"\"Run evaluation on all test cases in the dataset\"\"\"\n",
    "        with open(dataset_file, \"r\") as f:\n",
    "            dataset = json.load(f)\n",
    "\n",
    "        results = []\n",
    "        completed = 0\n",
    "        total = len(dataset)\n",
    "        last_reported_percentage = 0\n",
    "\n",
    "        with futures.ThreadPoolExecutor(\n",
    "            max_workers=self.max_concurrent_tasks\n",
    "        ) as executor:\n",
    "            future_to_test_case = {\n",
    "                executor.submit(\n",
    "                    self.run_test_case,\n",
    "                    test_case,\n",
    "                    run_prompt_function,\n",
    "                    extra_criteria,\n",
    "                ): test_case\n",
    "                for test_case in dataset\n",
    "            }\n",
    "\n",
    "            for future in futures.as_completed(future_to_test_case):\n",
    "                result = future.result()\n",
    "                completed += 1\n",
    "                current_percentage = int((completed / total) * 100)\n",
    "                milestone_percentage = (current_percentage // 20) * 20\n",
    "\n",
    "                if milestone_percentage > last_reported_percentage:\n",
    "                    print(f\"Graded {completed}/{total} test cases\")\n",
    "                    last_reported_percentage = milestone_percentage\n",
    "                results.append(result)\n",
    "\n",
    "        average_score = mean([result[\"score\"] for result in results])\n",
    "        print(f\"Average score: {average_score}\")\n",
    "\n",
    "        with open(json_output_file, \"w\") as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "\n",
    "        html = generate_prompt_evaluation_report(results)\n",
    "        with open(html_output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(html)\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Meal Plan Generation Example"
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T07:10:42.789841Z",
     "start_time": "2025-12-30T07:10:42.780193Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create an instance of PromptEvaluator\n",
    "# Increase `max_concurrent_tasks` for greater concurrency, but beware of rate limit errors!\n",
    "evaluator = PromptEvaluator(max_concurrent_tasks=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T07:10:53.572648Z",
     "start_time": "2025-12-30T07:10:43.809516Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1/3 test cases\n",
      "Generated 2/3 test cases\n",
      "Generated 3/3 test cases\n"
     ]
    }
   ],
   "source": [
    "dataset = evaluator.generate_dataset(\n",
    "    # Describe the purpose or goal of the prompt you're trying to test\n",
    "    task_description=\"Write a compact, concise 1 day meal plan for a single athlete\",\n",
    "    # Describe the different inputs that your prompt requires\n",
    "    prompt_inputs_spec={\n",
    "        \"height\": \"The height of the athlete in centimeters\",\n",
    "        \"weight\": \"The weight of the athlete in kilograms\",\n",
    "        \"goal\": \"The fitness goal of the athlete, e.g., 'build muscle', 'lose fat', 'maintain weight'\",\n",
    "        \"restrictions\": \"Any dietary restrictions the athlete has, e.g., 'vegetarian', 'gluten-free', 'nut allergy'\",\n",
    "    },\n",
    "    # Where to write the generated dataset\n",
    "    output_file=\"dataset_meals.json\",\n",
    "    # Number of test cases to generate (recommend keeping this low if you're getting rate limit errors)\n",
    "    num_cases=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T06:49:02.952035Z",
     "start_time": "2025-12-30T06:49:02.941327Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define and run the prompt you want to evaluate, returning the raw model output\n",
    "# This function is executed once for each test case\n",
    "\n",
    "# Prompting engineering best practices:\n",
    "# 1- Provide guidelines to control the output format\n",
    "# 2- Provide steps to help with complex problems\n",
    "# 3- Provide structure using xml tags when there's a lot of context.\n",
    "# 4- Provide examples (multishot prompting) when necessary, with explanations of why the example is ideal.\n",
    "def run_prompt(prompt_inputs: dict):\n",
    "    prompt = f\"\"\"\n",
    "    Generate a compact, concise 1 day meal plan for a single athlete with the following characteristics which meets their dietary restrictions and fitness goals.\n",
    "\n",
    "    <athlete_info>\n",
    "    - Height: {prompt_inputs[\"height\"]}\n",
    "    - Weight: {prompt_inputs[\"weight\"]}\n",
    "    - Goal: {prompt_inputs[\"goal\"]}\n",
    "    - Dietary restrictions: {prompt_inputs[\"restrictions\"]}\n",
    "    </athlete_info>\n",
    "\n",
    "    <guidelines>\n",
    "    Guidelines:\n",
    "    1. Include accurate daily calorie amount\n",
    "    2. Show protein, fat, and carb amounts\n",
    "    3. Specify when to eat each meal\n",
    "    4. Use only foods that fit restrictions\n",
    "    5. List all portion sizes in grams\n",
    "    6. Keep budget-friendly if mentioned\n",
    "    </guidelines>\n",
    "\n",
    "    Here is an example with input and an idea output\n",
    "\n",
    "    <sample_input>\n",
    "    height: 175\n",
    "    weight: 70\n",
    "    goal: maintain weight while staying energized for technique training\n",
    "    restrictions: none\n",
    "    </sample_input>\n",
    "\n",
    "    <ideal_output>\n",
    "    # 1-Day Meal Plan for Athlete\n",
    "\n",
    "    **Daily Targets:** 2,400 calories | 150g protein | 80g fat | 300g carbs\n",
    "\n",
    "    ---\n",
    "\n",
    "    ## **7:00 AM - Breakfast** (620 cal)\n",
    "    - Oatmeal: 80g dry\n",
    "    - Banana: 120g\n",
    "    - Peanut butter: 20g\n",
    "    - Whole milk: 250ml\n",
    "    - Whey protein: 30g\n",
    "\n",
    "    ---\n",
    "\n",
    "    ## **10:00 AM - Pre-Training Snack** (280 cal)\n",
    "    - Greek yogurt: 170g\n",
    "    - Honey: 15g\n",
    "    - Almonds: 20g\n",
    "\n",
    "    ---\n",
    "\n",
    "    ## **12:30 PM - Post-Training Lunch** (720 cal)\n",
    "    - Grilled chicken breast: 180g\n",
    "    - Brown rice: 100g dry weight\n",
    "    - Mixed vegetables: 150g\n",
    "    - Olive oil: 10g\n",
    "    - Apple: 150g\n",
    "\n",
    "    ---\n",
    "\n",
    "    ## **3:30 PM - Snack** (310 cal)\n",
    "    - Whole grain bread: 60g (2 slices)\n",
    "    - Turkey slices: 60g\n",
    "    - Avocado: 50g\n",
    "    - Cherry tomatoes: 100g\n",
    "\n",
    "    ---\n",
    "\n",
    "    ## **7:00 PM - Dinner** (680 cal)\n",
    "    - Salmon fillet: 150g\n",
    "    - Sweet potato: 200g\n",
    "    - Broccoli: 150g\n",
    "    - Olive oil: 10g\n",
    "\n",
    "    ---\n",
    "\n",
    "    ## **9:00 PM - Evening Snack** (190 cal)\n",
    "    - Cottage cheese: 150g\n",
    "    - Berries: 80g\n",
    "\n",
    "    ---\n",
    "\n",
    "    **Hydration:** 2.5-3L water throughout the day, especially around training\n",
    "    </ideal_output>\n",
    "\n",
    "    This example output is ideal because it fully satisfies all mandatory requirements by including daily caloric total, macronutrient breakdown, and meals with exact foods, portions, and timing. It also meets all secondary criteria: provides breakfast, lunch, dinner, and multiple snacks; total calories (2,400) are appropriate for the goal; and protein (150g) is adequate for muscle maintenance. The meal plan is well-structured for an athlete with technique training, includes proper hydration guidance, and aligns with the goal of maintaining weight while staying energized. No violations or significant deficiencies exist.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    messages = []\n",
    "    add_user_message(messages, prompt)\n",
    "    return chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T07:11:11.198605Z",
     "start_time": "2025-12-30T07:10:53.573515Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graded 1/3 test cases\n",
      "Graded 2/3 test cases\n",
      "Graded 3/3 test cases\n",
      "Average score: 9\n"
     ]
    }
   ],
   "source": [
    "results = evaluator.run_evaluation(\n",
    "    run_prompt_function=run_prompt,\n",
    "    dataset_file=\"dataset_meals.json\",\n",
    "    extra_criteria=\"\"\"\n",
    "     The output should include:\n",
    "    - Daily caloric total\n",
    "    - Macronutrient breakdown\n",
    "    - Meals with exact foods, portions, and timing\n",
    "    \"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Topic Extraction Example"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T06:58:23.051944Z",
     "start_time": "2025-12-30T06:58:23.022122Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create an instance of PromptEvaluator\n",
    "# Increase `max_concurrent_tasks` for greater concurrency, but beware of rate limit errors!\n",
    "topic_evaluator = PromptEvaluator(max_concurrent_tasks=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T06:58:41.184230Z",
     "start_time": "2025-12-30T06:58:24.527525Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1/4 test cases\n",
      "Generated 2/4 test cases\n",
      "Generated 3/4 test cases\n",
      "Generated 4/4 test cases\n"
     ]
    }
   ],
   "source": [
    "dataset = topic_evaluator.generate_dataset(\n",
    "    # Describe the purpose or goal of the prompt you're trying to test\n",
    "    task_description=\"\"\"\n",
    "    Extract topics out of a passage of text from a scholary article into a JSON array of strings\n",
    "    \"\"\",\n",
    "    # Describe the different inputs that your prompt requires\n",
    "    prompt_inputs_spec={\n",
    "        \"content\": \"The passage of text from the article to extract topics from written in Englishtopic_evaluator\"\n",
    "    },\n",
    "    # Where to write the generated dataset\n",
    "    output_file=\"dataset_topics.json\",\n",
    "    # Number of test cases to generate (recommend keeping this low if you're getting rate limit errors)\n",
    "    num_cases=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T07:10:14.726748Z",
     "start_time": "2025-12-30T07:10:14.717513Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define and run the prompt you want to evaluate, returning the raw model output\n",
    "# This function is executed once for each test case\n",
    "\n",
    "# Prompting engineering best practices:\n",
    "# 1- Provide guidelines to control the output format\n",
    "# 2- Provide steps to help with complex problems\n",
    "# 3- Provide structure using xml tags when there's a lot of context.\n",
    "# 4- Provide examples (multishot prompting) when necessary, with explanations of why the example is ideal.\n",
    "def run_topic_prompt(prompt_inputs: dict):\n",
    "    prompt = f\"\"\"\n",
    "    Extract key topics out of a passage of text from a scholarly article into a JSON array of strings.\n",
    "\n",
    "    <article>\n",
    "    {prompt_inputs[\"content\"]}\n",
    "    </article>\n",
    "\n",
    "    <guidelines>\n",
    "    Guidelines:\n",
    "    1. Accurately identify all main topics\n",
    "    2. Represent topics as concise strings\n",
    "    3. Output as a valid JSON array of strings\n",
    "    4. Do not include subtopics or unrelated themes\n",
    "    6. Do not include any explanation or extra text outside the JSON array\n",
    "    7. Ensure proper JSON formatting\n",
    "    8. Limit topics to those explicitly mentioned or clearly implied\n",
    "    9. Avoid overly broad or generic topics\n",
    "    </guidelines>\n",
    "\n",
    "    Here is an example with input and an idea output\n",
    "\n",
    "    <sample_input>\n",
    "    Ocean acidification represents a critical threat to marine ecosystems, driven by anthropogenic CO2 emissions that alter seawater chemistry. Our study employs coupled climate-ocean models to project pH changes across major oceanic regions through 2100 under various emission scenarios. Results indicate that without stringent mitigation policies aligned with the Paris Agreement targets, average surface ocean pH could decline by 0.3 units, threatening calcifying organisms including coral reefs and shellfish populations. The economic implications for fisheries-dependent communities are substantial, necessitating integrated policy frameworks that address both emission reduction strategies and adaptive management of marine resources. Our Bayesian statistical approach incorporates uncertainty quantification from ensemble model runs, providing probabilistic forecasts essential for evidence-based policymaking.\n",
    "    </sample_input>\n",
    "\n",
    "    <ideal_output>\n",
    "    [\n",
    "      \"Ocean acidification\",\n",
    "      \"Marine ecosystems\",\n",
    "      \"Anthropogenic CO2 emissions\",\n",
    "      \"Seawater chemistry\",\n",
    "      \"Climate-ocean models\",\n",
    "      \"pH changes\",\n",
    "      \"Emission scenarios\",\n",
    "      \"Paris Agreement\",\n",
    "      \"Calcifying organisms\",\n",
    "      \"Coral reefs\",\n",
    "      \"Shellfish populations\",\n",
    "      \"Fisheries economics\",\n",
    "      \"Climate policy\",\n",
    "      \"Emission reduction strategies\",\n",
    "      \"Marine resource management\",\n",
    "      \"Bayesian statistics\",\n",
    "      \"Uncertainty quantification\",\n",
    "      \"Ensemble modeling\",\n",
    "      \"Climate projections\",\n",
    "      \"Evidence-based policymaking\"\n",
    "    ]\n",
    "    </ideal_output>\n",
    "\n",
    "   \tThe solution meets all mandatory requirements: it is a valid JSON array of strings, contains topics from the article without extra commentary, and presents only the JSON array. It successfully identifies interdisciplinary topics spanning environmental policy (Paris Agreement, climate policy, emission reduction), data modeling (Bayesian statistics, uncertainty quantification, ensemble modeling), and oceanography (ocean acidification, seawater chemistry, marine ecosystems). All extracted topics are directly referenced or clearly implied in the source text. The minor weaknesses around granularity consistency do not violate any mandatory or secondary criteria.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    messages = []\n",
    "    add_user_message(messages, prompt)\n",
    "    return chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T07:09:39.952894Z",
     "start_time": "2025-12-30T07:09:24.539676Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graded 1/4 test cases\n",
      "Graded 2/4 test cases\n",
      "Graded 3/4 test cases\n",
      "Graded 4/4 test cases\n",
      "Average score: 9\n"
     ]
    }
   ],
   "source": [
    "results = topic_evaluator.run_evaluation(\n",
    "    run_prompt_function=run_topic_prompt,\n",
    "    dataset_file=\"dataset_topics.json\",\n",
    "    extra_criteria=\"\"\"\n",
    "    - Contains a JSON array of strings, containing each topic mentioned in the article.\n",
    "    - The strings should contain only a topic without any extra commentary\n",
    "    - Response should contain the JSON array and nothing else\n",
    "    \"\"\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
